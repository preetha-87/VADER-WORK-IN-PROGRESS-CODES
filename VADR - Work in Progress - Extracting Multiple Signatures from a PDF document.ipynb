{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in progress codes for VADR\n",
    "\n",
    "Based on idea formulated by: https://github.com/ahmetozlu\n",
    "\n",
    "This method is helpful in:\n",
    "a) Extracting overlapping signatures in a document\n",
    "b) Telling you how many people have signed the document when multiple signatures are required\n",
    "\n",
    "Entire Process is broken down into:\n",
    "\n",
    "a) Converting a PDF page into an image\n",
    "\n",
    "b) Reading the image\n",
    "\n",
    "c) Extracting blobs (textual prints within the document - including the signature) and identify their location\n",
    "\n",
    "d) Understanding the area of the text with a skimage library called regionprops, which uses the info on the area of the blobs - need to identify how much text is present in the image\n",
    "\n",
    "e) Create a threshold value - usually, a signature appears longer than the printed matter in the text - threshold is defined by the average value - i.e - size of the printed text - if the text (blob size) is greater than the average value, then this is an area of interest in the document - if the text (blob size) is lesser than the average value, then this is just noise \n",
    "\n",
    "average value = total area / number of blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from skimage import measure, morphology\n",
    "from skimage.color import label2rgb\n",
    "from skimage.measure import regionprops\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in a pdf file and convert it to an image\n",
    "from pdf2image import convert_from_path\n",
    "images=convert_from_path(\"C:/Users/preet/Desktop/dunnhumby_The-Complete-Journey/Sample.pdf\")\n",
    "for image in images:\n",
    "    image.save(\"Sample.jpeg\",\"jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " ...\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]\n",
      " [255 255 255 ... 255 255 255]]\n"
     ]
    }
   ],
   "source": [
    "# Read in the converted image\n",
    "#Use Open CV's imread to read the sample image\n",
    "img = cv2.imread('Sample.jpeg', 0)\n",
    "\n",
    "#The code below converts the image into a threshold binary image\n",
    "#We are interested in the black colour regions of the document\n",
    "#So, the values 127, 255 are pixel intensity values\n",
    "#127 is a global thresholding value\n",
    "#Thresholding is a technique in OpenCV, which is the assignment of pixel values in relation to the threshold value provided. \n",
    "#In thresholding, each pixel value is compared with the threshold value. \n",
    "#If the pixel value is smaller than the threshold, it is set to 0, otherwise, \n",
    "#it is set to a maximum value (generally 255). \n",
    "#Thresholding is a very popular segmentation technique, used for separating an object considered as a foreground \n",
    "#from its background. A threshold is a value which has two regions on its either side \n",
    "#i.e. below the threshold or above the threshold.\n",
    "#cv2.threshold(source, thresholdValue, maxVal, thresholdingTechnique)\n",
    "\n",
    "img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)[1] \n",
    "\n",
    "#Print it out to see if you are getting a multi-dimensional array of pixel values\n",
    "#The pixel values range from 0 to 255. That means 0 represents black and 255 represents white.\n",
    "#The range is 0-255 means that each pixel is represented by a single 8-bit integer.\n",
    "#Since the image is a colored image there are three channels. \n",
    "#Opencv reads the image in Blue Green Red(BGR) format.\n",
    "#So in the colored image, each pixel is represented by a three-element array, \n",
    "#with each integer representing one of the three color channels: B, G, and R, respectively.\n",
    "#Danger - if the object type produced by cv2.imread is none\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Extract Blobs\n",
    "#Need blob info to determine whether these blobs fulfill a criteria concerning the threshold value\n",
    "blobs = img > img.mean()\n",
    "blobs_labels = measure.label(blobs, background=1)\n",
    "#Convert these blobs into pixel values\n",
    "image_label_overlay = label2rgb(blobs_labels, image=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_biggest_component: 6147\n",
      "average: 198.21671826625388\n",
      "a4_constant: 689.9307091257556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Area of Text\n",
    "the_biggest_component = 0\n",
    "total_area = 0\n",
    "\n",
    "#counter denotes number of blobs\n",
    "#Extrcating information from the image - the area of the blobs\n",
    "#The code below captures total area of the blobs - trying to identify how much text is present in the image\n",
    "#Extracting this information is critical to creaate a threshold value\n",
    "#We are assuming that a person's signature is longer relative to the printed matter in the text\n",
    "counter = 0\n",
    "average = 0\n",
    "for region in regionprops(blobs_labels):\n",
    "    if (region.area > 10):\n",
    "        total_area = total_area + region.area\n",
    "        counter = counter + 1\n",
    "    if (region.area >= 250):\n",
    "        if (region.area > the_biggest_component):\n",
    "            the_biggest_component = region.area\n",
    "\n",
    "\n",
    "# Threshold\n",
    "#The average value is the size of the printed text\n",
    "average = (total_area/counter)\n",
    "print(\"the_biggest_component: \" + str(the_biggest_component))\n",
    "print(\"average: \" + str(average))\n",
    "\n",
    "#Use the average value is part of the formula to identify the location of the signature\n",
    "#The formula below pertains to documents of the size A4 (for a single A4 sized page)\n",
    "#This was compiled by: https://github.com/ahmetozlu\n",
    "#This a4_constant tells you how large the document's printed text actualy is\n",
    "\n",
    "a4_constant = ((average/84.0)*250.0)+100\n",
    "print(\"a4_constant: \" + str(a4_constant))\n",
    "\n",
    "#Call the remove_small_objects function that is part of the skimage library\n",
    "# Remove Noise - all those blobs that have a size that is lesser than the constant that was defined for A4 size documents\n",
    "#All parts of the printed text that are smaller than the constant for an A4 size sheet are removed. \n",
    "#blobs_labels\n",
    "\n",
    "b = morphology.remove_small_objects(blobs_labels, a4_constant)\n",
    "\n",
    "#Save the image with the noise removed\n",
    "\n",
    "plt.imsave('pre_version.jpeg', b)\n",
    "\n",
    "# read in the saved image that does not contain any noise\n",
    "\n",
    "img2 = cv2.imread('pre_version.jpeg', 0)\n",
    "img2 = cv2.threshold(img2, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# save the the result\n",
    "cv2.imwrite(\"output.jpeg\", img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(r\"C:\\Users\\preet\\Desktop\\output.jpeg\") \n",
    "  \n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
